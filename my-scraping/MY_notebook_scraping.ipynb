{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class USAJobsHTMLSpider(scrapy.Spider):\n",
    "    name = \"usajobs_batch_scraper\"\n",
    "\n",
    "    # Adjust concurrency settings\n",
    "    def __init__(self, job_ids, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.job_ids = job_ids\n",
    "        os.makedirs(\"job_html\", exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Start requests\n",
    "    def start_requests(self):\n",
    "        base_url = \"https://www.usajobs.gov/job/{}/print\"\n",
    "        for job_id in self.job_ids:\n",
    "            url = base_url.format(job_id)\n",
    "            yield scrapy.Request(\n",
    "                url, \n",
    "                callback=self.save_html, \n",
    "                meta={'job_id': job_id}, \n",
    "                errback=self.handle_error\n",
    "            )\n",
    "\n",
    "    # Save HTML\n",
    "    def save_html(self, response):\n",
    "        job_id = response.meta['job_id']\n",
    "        file_path = f\"job_html/usa_jobs_{job_id}.html\"\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "\n",
    "        self.log(f\"Saved: {file_path}\")\n",
    "\n",
    "    # Handle errors: if a job fails, save the job ID\n",
    "    def handle_error(self, failure):\n",
    "        \"\"\"Log failed job ID for later reprocessing.\"\"\"\n",
    "        job_id = failure.request.meta['job_id']\n",
    "        \n",
    "        with open(\"failed_jobID.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{job_id}\\n\")  # Save one failed job ID per line\n",
    "        \n",
    "        self.log(f\" Job ID {job_id} failed and saved to failed_jobID.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs: 2329297\n"
     ]
    }
   ],
   "source": [
    "### Download HTML\n",
    "\n",
    "\n",
    "# 1. read the control number from the txt file (extracted from the job DB)\n",
    "with open('job_control_number_list.txt') as f:\n",
    "    job_control_number_list = f.read().splitlines() \n",
    "job_control_number_list = [int(i) for i in job_control_number_list] # convert all job control number to int\n",
    "\n",
    "total_jobs_count = len(job_control_number_list) \n",
    "print(\"Total number of jobs:\", total_jobs_count)\n",
    "\n",
    "### potential chunking steps\n",
    "# # Split the list into batches of 1000\n",
    "# chunk_size = 1000\n",
    "# chunks = [job_control_number_list[i:i + chunk_size] for i in range(0, len(job_control_number_list), chunk_size)]\n",
    "# print(\"Split into\", len(chunks), \"chunks\", \"with size\", chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Run Scrapy with Optimized Settings\n",
    "def download_html_batch(job_ids, concurrent_requests=20):\n",
    "    process = CrawlerProcess(settings={\n",
    "        \"CONCURRENT_REQUESTS\": concurrent_requests,\n",
    "        \"DOWNLOAD_DELAY\": 2,  # Fixed delay (Scrapy auto-randomizes)\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,  # Dynamically adjusts request speed\n",
    "        \"AUTOTHROTTLE_START_DELAY\": 1,\n",
    "        \"AUTOTHROTTLE_MAX_DELAY\": 5,\n",
    "        \"LOG_LEVEL\": \"WARNING\",  # Less spammy logs, if want all, use INFO\n",
    "        \"COOKIES_ENABLED\": False,  # Reduces tracking risk\n",
    "        \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\"\n",
    "    })\n",
    "    \n",
    "    process.crawl(USAJobsHTMLSpider, job_ids=job_ids)\n",
    "    process.start()  # Start Scrapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 10:25:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/500620300/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:25:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/501366300/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:25:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/507491100/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:25:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/491506300/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:26:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/500665100/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:26:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/507657200/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:26:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/489972800/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:27:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/502248800/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:27:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/500018600/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:27:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/504223900/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:27:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/505569300/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-05 10:27:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/514583000/print> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n",
      "2025-03-07 17:54:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.usajobs.gov/job/520981300/print> (failed 3 times): 503 Service Unavailable\n"
     ]
    }
   ],
   "source": [
    "### Run Scrapy with Optimized Settings\n",
    "download_html_batch(job_control_number_list)  # Download all job HTMLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parse the HTML into JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse HTML to JSON\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def parse_html_to_json(job_id):\n",
    "    file_path = f\"job_html/usa_jobs_{job_id}.html\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"HTML file for job {job_id} not found! Please run download_html_batch() first.\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    # Extract Summary\n",
    "    summary_section = soup.find(\"div\", id=\"summary\")\n",
    "    # get the text only after the 'Summary' heading\n",
    "    summary = summary_section.get_text().split('Summary')[1].strip() if summary_section else \"Not available\"\n",
    "\n",
    "\n",
    "\n",
    "    # Extract Overview (all subsections)\n",
    "    overview_section = soup.find(\"div\", class_=\"usajobs-joa-section usajobs-joa-section-beta desktop-display-none\")\n",
    "    overview_dict = {}\n",
    "\n",
    "    if overview_section:\n",
    "        for item in overview_section.find_all(\"li\", class_=\"usajobs-joa-summary__item\"):\n",
    "            label = item.find(\"h3\")\n",
    "            value = item.find(\"p\")\n",
    "            if label and value:\n",
    "                overview_dict[label.get_text(strip=True)] = value.get_text(strip=True)\n",
    "\n",
    "    # Extract Hiring Paths\n",
    "    hiring_paths_section = soup.find(\"div\", id=\"hiring-paths\")\n",
    "    hiring_paths = [item.get_text(strip=True) for item in hiring_paths_section.find_all(\"div\", class_=\"usajobs-joa-intro-hiring-paths__title\")] if hiring_paths_section else []\n",
    "\n",
    "    # Extract Duties\n",
    "    duties_section = soup.find(\"div\", id=\"duties\")\n",
    "    duties = [li.get_text(strip=True) for li in duties_section.find_all(\"li\")] if duties_section else []\n",
    "\n",
    "    # Extract Requirements\n",
    "    requirements_section = soup.find(\"div\", id=\"requirements\")\n",
    "    requirements = [li.get_text(strip=True) for li in requirements_section.find_all(\"li\")] if requirements_section else []\n",
    "\n",
    "    # Extract How You Will Be Evaluated\n",
    "    evaluation_section = soup.find(\"div\", id=\"how-you-will-be-evaluated\")\n",
    "    evaluation_text = evaluation_section.get_text(strip=True) if evaluation_section else \"Not available\"\n",
    "\n",
    "    # Extract Required Documents\n",
    "    required_docs_section = soup.find(\"div\", id=\"required-documents\")\n",
    "    required_documents = [li.get_text(strip=True) for li in required_docs_section.find_all(\"li\")] if required_docs_section else []\n",
    "\n",
    "    # Structuring the extracted data into a JSON dictionary\n",
    "    job_data = {\n",
    "        \"job_id\": job_id,\n",
    "        \"summary\": summary,\n",
    "        \"overview\": overview_dict,\n",
    "        \"hiring_paths\": hiring_paths,\n",
    "        \"duties\": duties,\n",
    "        \"requirements\": requirements,\n",
    "        \"evaluation\": evaluation_text,\n",
    "        \"required_documents\": required_documents\n",
    "    }\n",
    "\n",
    "    # Save the extracted data to a JSON file\n",
    "    os.makedirs(\"job_json\", exist_ok=True) # create the directory if it doesn't exist\n",
    "    json_file_path = f\"job_json/usa_jobs_{job_id}.json\" \n",
    "    \n",
    "    # write the data to json file, and save to the job_json folder\n",
    "    with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file: \n",
    "        json.dump(job_data, json_file, indent=4) \n",
    "\n",
    "    print(f\"Saved JSON file: {json_file_path}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs: 30\n"
     ]
    }
   ],
   "source": [
    "### create a list called job_ids, including all the job control numbers in job_html folder\n",
    "import os\n",
    "folder_path = 'job_html'\n",
    "job_ids = []\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".html\"):\n",
    "        job_id = file.split(\"_\")[2].split(\".\")[0]\n",
    "        job_ids.append(job_id)\n",
    "\n",
    "\n",
    "print(\"Total number of jobs:\", len(job_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON file: job_json/usa_jobs_535784600.json\n",
      "Saved JSON file: job_json/usa_jobs_555186600.json\n",
      "Saved JSON file: job_json/usa_jobs_545139600.json\n",
      "Saved JSON file: job_json/usa_jobs_538360500.json\n",
      "Saved JSON file: job_json/usa_jobs_550621300.json\n",
      "Saved JSON file: job_json/usa_jobs_527728800.json\n",
      "Saved JSON file: job_json/usa_jobs_544120700.json\n",
      "Saved JSON file: job_json/usa_jobs_534103300.json\n",
      "Saved JSON file: job_json/usa_jobs_526191100.json\n",
      "Saved JSON file: job_json/usa_jobs_530191900.json\n",
      "Saved JSON file: job_json/usa_jobs_548471000.json\n",
      "Saved JSON file: job_json/usa_jobs_539292900.json\n",
      "Saved JSON file: job_json/usa_jobs_539216100.json\n",
      "Saved JSON file: job_json/usa_jobs_540226800.json\n",
      "Saved JSON file: job_json/usa_jobs_539293500.json\n",
      "Saved JSON file: job_json/usa_jobs_529651900.json\n",
      "Saved JSON file: job_json/usa_jobs_524360100.json\n",
      "Saved JSON file: job_json/usa_jobs_534788400.json\n",
      "Saved JSON file: job_json/usa_jobs_547650100.json\n",
      "Saved JSON file: job_json/usa_jobs_526456600.json\n",
      "Saved JSON file: job_json/usa_jobs_545865600.json\n",
      "Saved JSON file: job_json/usa_jobs_522233400.json\n",
      "Saved JSON file: job_json/usa_jobs_520636600.json\n",
      "Saved JSON file: job_json/usa_jobs_555153400.json\n",
      "Saved JSON file: job_json/usa_jobs_550153600.json\n",
      "Saved JSON file: job_json/usa_jobs_546737700.json\n",
      "Saved JSON file: job_json/usa_jobs_525321800.json\n",
      "Saved JSON file: job_json/usa_jobs_525004700.json\n",
      "Saved JSON file: job_json/usa_jobs_534735200.json\n",
      "Saved JSON file: job_json/usa_jobs_520693900.json\n"
     ]
    }
   ],
   "source": [
    "### Parse each HTML file into JSON\n",
    "for job_id in job_ids:\n",
    "    try :\n",
    "        parse_html_to_json(job_id) # parse the html file to json\n",
    "    except:\n",
    "        print(f\"Error parsing job {job_id}.\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clearai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
