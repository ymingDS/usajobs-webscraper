{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class USAJobsHTMLSpider(scrapy.Spider):\n",
    "    name = \"usajobs_batch_scraper\"\n",
    "\n",
    "    # Adjust concurrency settings\n",
    "    def __init__(self, job_ids, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.job_ids = job_ids\n",
    "        os.makedirs(\"job_html\", exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Start requests\n",
    "    def start_requests(self):\n",
    "        base_url = \"https://www.usajobs.gov/job/{}/print\"\n",
    "        for job_id in self.job_ids:\n",
    "            url = base_url.format(job_id)\n",
    "            yield scrapy.Request(\n",
    "                url, \n",
    "                callback=self.save_html, \n",
    "                meta={'job_id': job_id}, \n",
    "                errback=self.handle_error\n",
    "            )\n",
    "\n",
    "    # Save HTML\n",
    "    def save_html(self, response):\n",
    "        job_id = response.meta['job_id']\n",
    "        file_path = f\"job_html/usa_jobs_{job_id}.html\"\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "\n",
    "        self.log(f\"Saved: {file_path}\")\n",
    "\n",
    "    # Handle errors: if a job fails, save the job ID\n",
    "    def handle_error(self, failure):\n",
    "        \"\"\"Log failed job ID for later reprocessing.\"\"\"\n",
    "        job_id = failure.request.meta['job_id']\n",
    "        \n",
    "        with open(\"failed_jobID.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{job_id}\\n\")  # Save one failed job ID per line\n",
    "        self.log(f\" Job ID {job_id} failed and saved to failed_jobID.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicated job control numbers found.\n",
      "Total number of jobs: 1577395\n"
     ]
    }
   ],
   "source": [
    "# Read job control numbers from a file    \n",
    "with open('job_control_number_list.txt') as f:\n",
    "    job_control_number_list = f.read().splitlines() \n",
    "# make sure the job_control_number_list is unique\n",
    "job_control_number_list = list(set(job_control_number_list))\n",
    "\n",
    "# check if any job control number in this list are duplicated\n",
    "if len(job_control_number_list) != len(set(job_control_number_list)):\n",
    "    print(\"There are duplicated job control numbers in the list. Please check the input file.\")\n",
    "else:\n",
    "    print(\"No duplicated job control numbers found.\")\n",
    "\n",
    "total_jobs_count = len(job_control_number_list) \n",
    "print(\"Total number of jobs:\", total_jobs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Job IDs: 100%|██████████| 30/30 [01:13<00:00,  2.45s/job]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "class USAJobsHTMLSpider(scrapy.Spider):\n",
    "    name = \"usajobs_batch_scraper\"\n",
    "\n",
    "    # Adjust concurrency settings\n",
    "    def __init__(self, job_ids, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.job_ids = job_ids\n",
    "        os.makedirs(\"job_html\", exist_ok=True)  # Ensure directory exists\n",
    "        self.progress_bar = tqdm(total=len(job_ids), desc=\"Scraping Job IDs\", unit=\"job\")  # Initialize progress bar\n",
    "\n",
    "    # Start requests\n",
    "    def start_requests(self):\n",
    "        base_url = \"https://www.usajobs.gov/job/{}/print\"\n",
    "        for job_id in self.job_ids:\n",
    "            url = base_url.format(job_id)\n",
    "            yield scrapy.Request(\n",
    "                url, \n",
    "                callback=self.save_html, \n",
    "                meta={'job_id': job_id}, \n",
    "                errback=self.handle_error\n",
    "            )\n",
    "\n",
    "    # Save HTML\n",
    "    def save_html(self, response):\n",
    "        job_id = response.meta['job_id']\n",
    "        file_path = f\"job_html/usa_jobs_{job_id}.html\"\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "\n",
    "        self.log(f\"Saved: {file_path}\")\n",
    "        self.progress_bar.update(1)  # Update progress bar after each job is saved\n",
    "\n",
    "    # Handle errors: if a job fails, save the job ID\n",
    "    def handle_error(self, failure):\n",
    "        \"\"\"Log failed job ID for later reprocessing.\"\"\"\n",
    "        job_id = failure.request.meta['job_id']\n",
    "        \n",
    "        with open(\"failed_jobID.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{job_id}\\n\")  # Save one failed job ID per line\n",
    "        \n",
    "        self.log(f\" Job ID {job_id} failed and saved to failed_jobID.txt\")\n",
    "        self.progress_bar.update(1)  # Update progress bar even for failed jobs\n",
    "\n",
    "    def closed(self, reason):\n",
    "        \"\"\"Close the progress bar when the spider is done.\"\"\"\n",
    "        self.progress_bar.close()\n",
    "\n",
    "# Function to Run Scrapy with Optimized Settings\n",
    "def download_html_batch(job_ids, concurrent_requests=20):\n",
    "    process = CrawlerProcess(settings={\n",
    "        \"CONCURRENT_REQUESTS\": concurrent_requests,\n",
    "        \"DOWNLOAD_DELAY\": 2,  # Fixed delay (Scrapy auto-randomizes)\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,  # Dynamically adjusts request speed\n",
    "        \"AUTOTHROTTLE_START_DELAY\": 1,\n",
    "        \"AUTOTHROTTLE_MAX_DELAY\": 5,\n",
    "        \"LOG_LEVEL\": \"WARNING\",  # Less spammy logs, if want all, use INFO\n",
    "        \"COOKIES_ENABLED\": False,  # Reduces tracking risk\n",
    "        \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\"\n",
    "    })\n",
    "    \n",
    "    process.crawl(USAJobsHTMLSpider, job_ids=job_ids)\n",
    "    process.start()  # Start Scrapy\n",
    "\n",
    "# Read job control numbers from a file    \n",
    "with open('job_control_number_list_30.txt') as f:\n",
    "    job_control_number_list = f.read().splitlines() \n",
    "job_control_number_list = [int(i) for i in job_control_number_list] # convert all job control number to int\n",
    "\n",
    "\n",
    "total_jobs_count = len(job_control_number_list) \n",
    "print(\"Total number of jobs:\", total_jobs_count)\n",
    "download_html_batch(job_control_number_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clearai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
